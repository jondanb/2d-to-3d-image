{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGmhbrqtpIfB"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "\n",
        "!pip install torch\n",
        "!pip install opencv-python\n",
        "!pip install numpy\n",
        "!pip install timm\n",
        "!pip install tqdm\n",
        "!pip install jax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6Mvf530QpU_u"
      },
      "outputs": [],
      "source": [
        "# Set up imports\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import tqdm\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "R-290ivXqjUn"
      },
      "outputs": [],
      "source": [
        "VW_FOURCC_CODEC = \"MP4V\"\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def depth_video(filename: str, output_folder: str):\n",
        "    capture = cv2.VideoCapture(filename)\n",
        "\n",
        "    frame_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    frame_rate = int(capture.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*VW_FOURCC_CODEC)\n",
        "\n",
        "    progress_bar = tqdm.tqdm(total=frame_count, position=0, leave=True)\n",
        "    out = cv2.VideoWriter(\"output3d.mp4\", fourcc, frame_rate, (width, height))\n",
        "\n",
        "    #frame_idx = 0\n",
        "\n",
        "    while capture.isOpened():\n",
        "        ret, frame = capture.read()\n",
        "\n",
        "        if frame is not None:\n",
        "            #depth_frame = depth_image(frame)\n",
        "            #depth_frame = cv2.normalize(src=depth_frame, dst=None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
        "\n",
        "            #bit_mask = cv2.inRange(depth_frame, 180, 255)\n",
        "            #top_layer = cv2.cvtColor(bit_mask, cv2.COLOR_GRAY2BGR)\n",
        "            #masked = cv2.bitwise_and(frame, top_layer)\n",
        "\n",
        "            #new_frame = _shift_image_overlay(frame, masked, bit_mask, 15)\n",
        "            #cv2_imshow(new_frame)\n",
        "\n",
        "            #cv2.imwrite(os.path.join(output_folder, format(frame_idx, \"08\")) + \".png\", depth_frame)\n",
        "            new_frame = cv2.cvtColor(convert_image_2d_to_3d(frame), cv2.COLOR_RGB2BGR)\n",
        "            out.write(new_frame)\n",
        "            #cv2_imshow(new_frame)\n",
        "            progress_bar.update(1)\n",
        "            #frame_idx += 1\n",
        "\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    #!ffmpeg -framerate $frame_rate -i '/content/out/%08d.png' /content/output.mp4\n",
        "\n",
        "    capture.release()\n",
        "    out.release()\n",
        "    progress_bar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1CQ16tPQp6-9"
      },
      "outputs": [],
      "source": [
        "def depth_image(img: np.ndarray):\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    input_batch = transform(img).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        prediction = midas(input_batch)\n",
        "\n",
        "        prediction = torch.nn.functional.interpolate(\n",
        "            prediction.unsqueeze(1),\n",
        "            size=img.shape[:2],\n",
        "            mode=\"bicubic\",\n",
        "            align_corners=False,\n",
        "        ).squeeze()\n",
        "\n",
        "    return prediction.cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ijNIc3Z_6ciZ"
      },
      "outputs": [],
      "source": [
        "def convert_image_2d_to_3d(image):\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "  input_batch = transform(image).to(device)\n",
        "\n",
        "  torch.set_grad_enabled(False)\n",
        "  prediction = midas(input_batch)\n",
        "\n",
        "  prediction = torch.nn.functional.interpolate(\n",
        "      prediction.unsqueeze(1),\n",
        "      size=image.shape[:2],\n",
        "      mode=\"bicubic\",\n",
        "      align_corners=False,\n",
        "  ).squeeze()\n",
        "  torch.set_grad_enabled(True)\n",
        "\n",
        "  output = prediction.cpu().numpy()\n",
        "\n",
        "  new_image = image.copy()\n",
        "  depth_frame = cv2.normalize(src=output, dst=None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
        "\n",
        "  mesh_x, mesh_y = np.meshgrid(np.arange(depth_frame.shape[1]), np.arange(depth_frame.shape[0]))\n",
        "  new_mesh_x = mesh_x - (((255 - depth_frame) // 13) + 7)\n",
        "\n",
        "  new_image[mesh_y, new_mesh_x, 0] = image[mesh_y, mesh_x, 0]#cv2.GaussianBlur(image[mesh_y, mesh_x, 0], (7, 7), cv2.BORDER_DEFAULT)\n",
        "\n",
        "  return new_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Zp9DnXcXpwPt"
      },
      "outputs": [],
      "source": [
        "# Borrowed from https://colab.research.google.com/github/pytorch/pytorch.github.io/blob/master/assets/hub/intelisl_midas_v2.ipynb\n",
        "\n",
        "# @markdown ---\n",
        "#@markdown ### Enter the model to use for depth estimation\n",
        "# @markdown ##### **DPT_Large**: MiDaS v3 - Large *(highest accuracy, slowest inference speed)*\n",
        "# @markdown ##### **DPT_Hybrid**: MiDaS v3 - Hybrid *(medium accuracy, medium inference speed)*\n",
        "# @markdown ##### **MiDaS_small**: MiDaS v2.1 - Small *(lowest accuracy, highest inference speed)*\n",
        "model_type = \"DPT_Large\" # @param [\"DPT_Large\", \"DPT_Hybrid\", \"MiDaS_small\"]\n",
        "# @markdown ---\n",
        "#@markdown ### Enter an image path to convert to 3D:\n",
        "image_filename = \"IMG_3892.PNG\" #@param {type:\"string\"}\n",
        "# @markdown ##### Or if you are trying to convert a video:\n",
        "video_filename = \"\" #@param {type:\"string\"}\n",
        "# @markdown ---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "midas = torch.hub.load(\"intel-isl/MiDaS\", model_type)\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "midas.to(device)\n",
        "midas.eval()\n",
        "\n",
        "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
        "\n",
        "if model_type == \"DPT_Large\" or model_type == \"DPT_Hybrid\":\n",
        "    transform = midas_transforms.dpt_transform\n",
        "else:\n",
        "    transform = midas_transforms.small_transform"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juqOMvX0dSj6",
        "outputId": "b34cfcd2-23b1-4a2b-a6c0-2379bb0c20ab"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/intel-isl_MiDaS_master\n",
            "Using cache found in /root/.cache/torch/hub/intel-isl_MiDaS_master\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xv6arkMBJJo_"
      },
      "outputs": [],
      "source": [
        "if image_filename:\n",
        "  image = cv2.imread(image_filename)\n",
        "  image_3d = cv2.cvtColor(convert_image_2d_to_3d(image), cv2.COLOR_RGB2BGR)\n",
        "  cv2_imshow(image_3d)\n",
        "elif video_filename:\n",
        "  os.path.makedirs(\"/content/out\")\n",
        "  print(\"Video output to /content/out\")\n",
        "  depth_video(video_filename, \"/content/out\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NhA2o7QsrSd"
      },
      "outputs": [],
      "source": [
        "# @markdown # Extra plt display\n",
        "img = cv2.imread(image_filename)\n",
        "\n",
        "input_batch = transform(img).to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    prediction = midas(input_batch)\n",
        "\n",
        "    prediction = torch.nn.functional.interpolate(\n",
        "        prediction.unsqueeze(1),\n",
        "        size=img.shape[:2],\n",
        "        mode=\"bicubic\",\n",
        "        align_corners=False,\n",
        "    ).squeeze()\n",
        "\n",
        "output = prediction.cpu().numpy()\n",
        "\n",
        "fig, axarr = plt.subplots(1, 3, figsize=(30, 30))\n",
        "\n",
        "axarr[0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "axarr[1].imshow(output, cmap=\"viridis\", interpolation=\"nearest\")\n",
        "axarr[2].imshow(convert_image_2d_to_3d(img))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}